# GradientVisualizer

## In this project, I am exploring the behavior of gradients in an overly simple "Neural Networks" and a dynamic batch size prototype.

## I am also looking into how different activation functions, gradient functions, and number of "nodes" effect the convergence of weights and biases.

## The goal is to get a better understanding of why certain functions work and why others don't as well as if other basic techniques such as "phantom gradients" and dynamic batch sizes based on the noise of the gradient can accelerate convergence with better results.
